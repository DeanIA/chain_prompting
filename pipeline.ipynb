{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ee6fcfc",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b190a351",
   "metadata": {},
   "source": [
    "# Context Prompting with Internvl 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497dae38",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install the environment\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0355ad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import torch\n",
    "import traceback\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import torchvision.transforms as T\n",
    "from decord import VideoReader, cpu\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd26e577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Runtime Parameters ────────────────────────────────────────────────\n",
    "NUM_FRAMES = 32          # frames sampled per clip\n",
    "CLIP_DURATION = 10       # seconds per segment\n",
    "TEMPERATURE = 0.1\n",
    "\n",
    "INPUT_SIZE = 448\n",
    "MAX_PATCHES = 12\n",
    "\n",
    "\n",
    "GEN_CONFIG = {\n",
    "    \"max_new_tokens\": 225,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": TEMPERATURE\n",
    "}\n",
    "\n",
    "VIDEO_EXTENSIONS = [\n",
    "    \".mp4\", \".mov\", \".avi\", \".mkv\", \".flv\", \".wmv\", \".webm\",\n",
    "    \".mpeg\", \".mpg\", \".m4v\", \".3gp\", \".3g2\", \".mts\", \".m2ts\",\n",
    "    \".ts\", \".ogv\", \".divx\", \".vob\", \".rm\", \".rmvb\", \".asf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7baa192b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.60s/it]\n"
     ]
    }
   ],
   "source": [
    "# ─── Quantized Model ─────────────────────────────────────────────────────────────\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "path = \"OpenGVLab/InternVL3_5-8B\"\n",
    "model = AutoModel.from_pretrained(\n",
    "    path,\n",
    "    quantization_config=bnb_config,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_flash_attn=True,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"cuda:1\"\n",
    ").eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c951f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Preprocessing ─────────────────────────────────────────────────────\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    return T.Compose([\n",
    "        T.Lambda(lambda img: img.convert(\"RGB\") if img.mode != \"RGB\" else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "    ])\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "\n",
    "def get_index(bound, fps, max_frame, first_idx=0, num_segments=NUM_FRAMES):\n",
    "    if bound:\n",
    "        start, end = bound[0], bound[1]\n",
    "    else:\n",
    "        start, end = -100000, 100000\n",
    "    start_idx = max(first_idx, round(start * fps))\n",
    "    end_idx = min(round(end * fps), max_frame)\n",
    "    seg_size = float(end_idx - start_idx) / num_segments\n",
    "    frame_indices = np.array([\n",
    "        int(start_idx + (seg_size / 2) + np.round(seg_size * idx))\n",
    "        for idx in range(num_segments)\n",
    "    ])\n",
    "    return frame_indices\n",
    "\n",
    "def load_video(video_path, bound=None, input_size=INPUT_SIZE, max_num=1, num_segments=NUM_FRAMES):\n",
    "    vr = VideoReader(video_path, ctx=cpu(0), num_threads=1)\n",
    "    max_frame = len(vr) - 1\n",
    "    fps = float(vr.get_avg_fps())\n",
    "\n",
    "    pixel_values_list, num_patches_list = [], []\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    frame_indices = get_index(bound, fps, max_frame, first_idx=0, num_segments=num_segments)\n",
    "\n",
    "    for frame_index in frame_indices:\n",
    "        img = Image.fromarray(vr[frame_index].asnumpy()).convert('RGB')\n",
    "        img = dynamic_preprocess(img, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "        pixel_values = [transform(tile) for tile in img]\n",
    "        pixel_values = torch.stack(pixel_values)\n",
    "        num_patches_list.append(pixel_values.shape[0])\n",
    "        pixel_values_list.append(pixel_values)\n",
    "\n",
    "    pixel_values = torch.cat(pixel_values_list)\n",
    "    return pixel_values, num_patches_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb3fa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Prompt Helpers ────────────────────────────────────────────────────\n",
    "def clip_as_prompt(num_patches_list):\n",
    "    # One <image> token per frame/tile group\n",
    "    return \"\\n\".join(f\"Frame{i+1}: <image>\" for i in range(len(num_patches_list)))\n",
    "\n",
    "\n",
    "def build_prompt(start, num_patches_list, prev_output, first_prompt, second_prompt):\n",
    "    \"\"\"Builds the text prompt for a video segment.\"\"\"\n",
    "    if start == 0:\n",
    "        return f\"{clip_as_prompt(num_patches_list)}\\n{first_prompt}\"\n",
    "    \n",
    "    context = f\"The previous clip showed: {prev_output}\\n\"\n",
    "    frames = clip_as_prompt(num_patches_list)\n",
    "    return f\"{context}{second_prompt}\\n{frames}\"\n",
    "\n",
    "\n",
    "# ─── Inference ─────────────────────────────────────────────────────────\n",
    "def load_video_metadata(video_file_path):\n",
    "    try:\n",
    "        reader = VideoReader(str(video_file_path), ctx=cpu(0))\n",
    "        fps = reader.get_avg_fps()\n",
    "        duration = len(reader) / fps\n",
    "        return fps, duration\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def process_video_segment(video_path, start, end, prev_output, first_prompt, second_prompt, system_prompt):\n",
    "    print(f\" Segment {start:>5.1f}s – {end:>6.1f}s\")\n",
    "\n",
    "    bound = (start, end)\n",
    "    vid_t, num_patches_list = load_video(str(video_path), bound=bound)\n",
    "    if vid_t.numel() == 0:\n",
    "        return None, prev_output\n",
    "\n",
    "    vid_t = vid_t.to(device=model.device, dtype=torch.float16).contiguous()\n",
    "    prompt = build_prompt(start, num_patches_list, prev_output, first_prompt, second_prompt)\n",
    "\n",
    "    model.system_message = system_prompt.strip()\n",
    "    response, _ = model.chat(\n",
    "        tokenizer, vid_t, prompt, GEN_CONFIG,\n",
    "        history=None, return_history=True,\n",
    "        num_patches_list=num_patches_list\n",
    "    )\n",
    "\n",
    "    del vid_t\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def describe_video(video_file_path, first_prompt, second_prompt, system_prompt, segment_duration=CLIP_DURATION):\n",
    "    summaries, prev_output = [], \"\"\n",
    "    fps, total_duration = load_video_metadata(video_file_path)\n",
    "    if fps is None:\n",
    "        return []\n",
    "\n",
    "    print(f\"Processing {Path(video_file_path).name} ({total_duration:.1f}s)\")\n",
    "    duration_int = int(total_duration + 1e-6)\n",
    "\n",
    "    for start in range(0, duration_int, segment_duration):\n",
    "        end = min(start + segment_duration, duration_int)\n",
    "        seg_summary, prev_output = process_video_segment(\n",
    "            video_file_path, start, end, prev_output, first_prompt, second_prompt, system_prompt\n",
    "        )\n",
    "        if seg_summary:\n",
    "            summaries.append(seg_summary)\n",
    "\n",
    "    return summaries\n",
    "\n",
    "\n",
    "def generate_video_descriptions(media_dir, first_prompt, second_prompt, system_prompt):\n",
    "    results = {}\n",
    "    for filename in sorted(os.listdir(media_dir)):\n",
    "        path = os.path.join(media_dir, filename)\n",
    "        if filename.lower().endswith(tuple(VIDEO_EXTENSIONS)):\n",
    "            clips = describe_video(path, first_prompt, second_prompt, system_prompt)\n",
    "            final = clips[-1][\"description\"] if clips else \"\"\n",
    "            results[filename] = final\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8c0b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Processing TNS_0002_V.MP4 (29.5s)\n",
      " Segment   0.0s –   10.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Segment  10.0s –   20.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Segment  20.0s –   29.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "FIRST_PROMPT = \"Describe in detail what is happening in this video.\"\n",
    "SECOND_PROMPT = \"Use the descriptions from previous clip to generate a cumulative of the this clip from the same video.\"\n",
    "\n",
    "R1_SYSTEM_PROMPT = '''\n",
    "You are an AI assistant that rigorously follows this response protocol:\n",
    "\n",
    "1. First, conduct a detailed analysis of the question. Consider different angles, potential solutions, and reason through the problem step-by-step. Enclose this entire thinking process within <think> and </think> tags.\n",
    "\n",
    "2. After the thinking section, provide a clear, concise, and direct answer to the user's question. Separate the answer from the think section with a newline.\n",
    "\n",
    "Ensure that the thinking process is thorough but remains focused on the query. The final answer should be standalone and not reference the thinking section.\n",
    "'''.strip()\n",
    "\n",
    "descriptions = generate_video_descriptions(\"toy_ds\", FIRST_PROMPT, SECOND_PROMPT, R1_SYSTEM_PROMPT)\n",
    "with open(\"video_descriptions.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(descriptions, f, indent=2, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intern_vl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
